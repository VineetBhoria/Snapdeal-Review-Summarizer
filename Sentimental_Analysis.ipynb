{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXH5wOTl6SwA",
        "outputId": "a61441b0-1284-4979-cb25-b97d7ab6899a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.13.1)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"
          ]
        }
      ],
      "source": [
        "!pip install playwright\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import asyncio\n",
        "import time\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "nest_asyncio.apply()  # Allowing nested async execution inside Colab\n",
        "\n",
        "async def get_snapdeal_reviews(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        try:\n",
        "            await page.goto(url, timeout=90000)\n",
        "\n",
        "            # Extracting product name with error handling\n",
        "            try:\n",
        "                product_name = await page.locator(\"h1\").text_content(timeout=60000)\n",
        "                product_name = product_name.strip() if product_name else \"Unknown Product\"\n",
        "            except Exception:\n",
        "                product_name = \"Unknown Product\"\n",
        "\n",
        "            # Waiting for reviews to load\n",
        "            await page.wait_for_load_state(\"domcontentloaded\", timeout=60000)\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "            reviews = []\n",
        "            while True:\n",
        "                # Extracting reviews from the current page\n",
        "                page_reviews = await page.locator(\".user-review\").all_text_contents()\n",
        "                reviews.extend(page_reviews)\n",
        "\n",
        "                # Checking if \"Next\" button exists for more reviews\n",
        "                next_button = await page.locator(\"button[aria-label='Next']\").is_visible()\n",
        "                if next_button:\n",
        "                    await page.locator(\"button[aria-label='Next']\").click()\n",
        "                    await page.wait_for_timeout(5000)  # Wait for the next page to load\n",
        "                else:\n",
        "                    break  # No more pages, exit loop\n",
        "\n",
        "            await browser.close()\n",
        "\n",
        "            # Removing duplicates\n",
        "            reviews = list(set(reviews))\n",
        "\n",
        "            return product_name, reviews\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {url}: {str(e)}\")\n",
        "            await browser.close()\n",
        "            return None, []\n",
        "\n",
        "# Function to clean reviews\n",
        "def clean_review(text):\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'by .* on \\w+ \\d{2}, \\d{4} Verified Buyer', '', text)\n",
        "\n",
        "    # Removing date-only reviews\n",
        "    if re.fullmatch(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}|\\w{3,9} \\d{1,2}, \\d{4}', text):\n",
        "        return None\n",
        "\n",
        "    # Removing dates that are inside the review\n",
        "    text = re.sub(r'\\b\\w{3,9} \\d{1,2}, \\d{4}\\b', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# List of 50 product URLs (unchanged)\n",
        "product_urls = [\n",
        "\"https://www.snapdeal.com/product/arni-3-in-1-large/658713912149#bcrumbSearch:kitchen\",\n",
        "\"https://www.snapdeal.com/product/analog-kitchenware-dark-grey-aluminium/686220079278\",\n",
        "\"https://www.snapdeal.com/product/sanjana-silk-grey-georgette-saree/8070451156087011442\",\n",
        "\"https://www.snapdeal.com/product/sherine-purple-georgette-saree-with/5764608142053716272#bcrumbLabelId:176\",\n",
        "\"https://www.snapdeal.com/product/sitanjali-red-satin-saree-single/653147619600\",\n",
        "\"https://www.snapdeal.com/product/gazal-fashions-red-banarasi-silk/5188147395679512622#bcrumbLabelId:176\",\n",
        "\"https://www.snapdeal.com/product/rage-gaze-pu-brown-casual/632657477424\",\n",
        "\"https://www.snapdeal.com/product/samtroh-pu-beige-mens-regular/648790735271#bcrumbLabelId:46139355\",\n",
        "\"https://www.snapdeal.com/product/topware-faux-leather-brown-casual/618782211099#bcrumbLabelId:46139355\",\n",
        "\"https://www.snapdeal.com/product/sambhav-deals-pu-tan-formal/670993774775#bcrumbLabelId:46139355\",\n",
        "\"https://www.snapdeal.com/product/glorious-pink-cotton-aline-kurti/663061347291\",\n",
        "\"https://www.snapdeal.com/product/radiksa-turquoise-cotton-straight-kurti/618611089447\",\n",
        "\"https://www.snapdeal.com/product/hetsa-rust-cotton-blend-straight/8646911954626999414\",\n",
        "\"https://www.snapdeal.com/product/beauty-berry-3-in1-long/632437546114\",\n",
        "\"https://www.snapdeal.com/product/comey-cotton-blend-pink-solids/6341068901840999593\",\n",
        "\"https://www.snapdeal.com/product/herbalife-200g-personalized-protein-powder/639121239022\",\n",
        "\"https://www.snapdeal.com/product/swiss-beauty-liquid-foundation-light/8070451160909091139\",\n",
        "\"https://www.snapdeal.com/product/analog-kichenware-utility-knife-3/5764608148853588121\",\n",
        "\"https://www.snapdeal.com/product/latest-chikan-cotton-blend-yellow/6917529706562416229#bcrumbLabelId:191\",\n",
        "\"https://www.snapdeal.com/product/deshbandhu-dbk-100-percent-cotton/663003928606#bcrumbLabelId:191\",\n",
        "\"https://www.snapdeal.com/product/vida-loca-linen-maroon-shirt/5188147447065960051#bcrumbLabelId:191\",\n",
        "\"https://www.snapdeal.com/product/campus-terminator-n-blue-running/8070451213417691561#bcrumbLabelId:255\",\n",
        "\"https://www.snapdeal.com/product/asian-lifestyle-gray-casual-shoes/6917529696477217208#bcrumbLabelId:255\",\n",
        "\"https://www.snapdeal.com/product/allamwar-12pcs-stainless-steel-cookie/5764608156150767097\",\n",
        "\"https://www.snapdeal.com/product/2-in-1-soap-pump/646826407477\",\n",
        "\"https://www.snapdeal.com/product/stainless-steel-vestire-powerfree-hand/5764608202055287770\",\n",
        "\"https://www.snapdeal.com/product/flyfot-kreative-india-plastic-multipurpose/6917529700943501959\",\n",
        "\"https://www.snapdeal.com/product/plastic-quick-cutter-vegetable-cutter/4899917027919989123\",\n",
        "\"https://www.snapdeal.com/product/shray-32-in-1-interchangeble/683655001564#bcrumbSearch:laptops\",\n",
        "\"https://www.snapdeal.com/product/bhawna-collection-loard-shiv-trishul/672311651336\",\n",
        "\"https://www.snapdeal.com/product/milton-thermosteel-1000-ml-flask/1383039\",\n",
        "\"https://www.snapdeal.com/product/prd-pu-tan-casual-long/634235191285\",\n",
        "\"https://www.snapdeal.com/product/tantra-fluke-car-bluetooth-kit/675090421220#bcrumbLabelId:46102495\",\n",
        "\"https://www.snapdeal.com/product/dynamic-store-stainless-steel-kitchen/718714840#bcrumbSearch:kitchen\",\n",
        "\"https://www.snapdeal.com/product/masala-rangoli-box-dabba-for/648082979993#bcrumbSearch:kitchen\",\n",
        "\"https://www.snapdeal.com/product/hometales-polyproplene-food-container-set/6917529686805260778#bcrumbSearch:kitchen\",\n",
        "\"https://www.snapdeal.com/product/kitchen-shelf-storage-rack-self/651274312796#bcrumbSearch:kitchen\",\n",
        "\"https://www.snapdeal.com/product/14-in-1-push-up/656865898041#bcrumbLabelId:777\",\n",
        "\"https://www.snapdeal.com/product/swiss-beauty-face-primer-cream/643865380404#bcrumbLabelId:3711\",\n",
        "\"https://www.snapdeal.com/product/jawline-exerciser-tool/674117063799#bcrumbLabelId:777\",\n",
        "\"https://www.snapdeal.com/product/boldfit-push-up-bar-stand/644707316809#bcrumbLabelId:777\",\n",
        "\"https://www.snapdeal.com/product/double-spring-tummy-trimmer-pro/623249262781#bcrumbLabelId:777\",\n",
        "\"https://www.snapdeal.com/product/swiss-beauty-professional-warm-sand/6917529682500675450#bcrumbLabelId:3711\",\n",
        "\"https://www.snapdeal.com/product/sweat-belt-hot-shapers-hot/655284657487#bcrumbLabelId:777\",\n",
        "\"https://www.snapdeal.com/product/unical-shock-resistant-cable-protector/643117396448#bcrumbSearch:smartphones\",\n",
        "\"https://www.snapdeal.com/product/elv-foldable-universal-tablet-phone/5764608203624461851#bcrumbSearch:smartphones\",\n",
        "\"https://www.snapdeal.com/product/hybite-premium-selfie-stick-blue/638464915190#bcrumbSearch:smartphones\",\n",
        "\"https://www.snapdeal.com/product/nutriley-un-flavoured-mass-gainer/638070292001#bcrumbSearch:mass%20gainer|bcrumbLabelId:46101962\",\n",
        "\"https://www.snapdeal.com/product/intimify-kesar-pista-badam-mass/622640821970#bcrumbSearch:whey|bcrumbLabelId:46101962\",\n",
        "\"https://www.snapdeal.com/product/london-glow-setting-powder-white/666659763053#bcrumbSearch:protien|bcrumbLabelId:46101962\",\n",
        "]\n",
        "\n",
        "\n",
        "# Run the async function for all products\n",
        "all_reviews = []\n",
        "for url in product_urls:\n",
        "    product_name, reviews = asyncio.get_event_loop().run_until_complete(get_snapdeal_reviews(url))\n",
        "\n",
        "    # Clean reviews and filter out empty ones\n",
        "    cleaned_reviews = [clean_review(review) for review in reviews]\n",
        "    cleaned_reviews = [review for review in cleaned_reviews if review]\n",
        "\n",
        "    # Keep only 10 reviews per product\n",
        "    cleaned_reviews = cleaned_reviews[:10]\n",
        "\n",
        "    for review in cleaned_reviews:\n",
        "        all_reviews.append({\n",
        "            \"Product Name\": product_name,\n",
        "            \"Reviews\": review,\n",
        "            # Removed the Useful_in_India placeholder from here\n",
        "        })\n",
        "\n",
        "    # Add a delay between products to avoid rate-limiting\n",
        "    time.sleep(random.uniform(5, 15))\n",
        "\n",
        "# Saving to CSV\n",
        "df = pd.DataFrame(all_reviews)\n",
        "csv_filename = \"Snapdeal_Product_Reviews.csv\"\n",
        "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"✅ Product reviews saved to: {csv_filename}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UiWcpTZ76iG1",
        "outputId": "7e679706-19a9-428b-95bb-77cc7fc00a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Product reviews saved to: Snapdeal_Product_Reviews.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b7b454f3-8011-432c-a960-f45fb8959e26\", \"Snapdeal_Product_Reviews.csv\", 72673)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the existing CSV file containing reviews\n",
        "df = pd.read_csv(\"Snapdeal_Product_Reviews.csv\")\n",
        "\n",
        "# Define keywords for sentiment analysis\n",
        "positive_keywords = [\"good\", \"great\", \"perfect\", \"useful\", \"value\", \"worth\", \"working\", \"excellent\", \"satisfied\", \"awesome\", \"nice\", \"recommend\", \"helpful\"]\n",
        "negative_keywords = [\"bad\", \"worst\", \"not working\", \"poor\", \"useless\", \"waste\", \"broke\", \"disappointed\", \"damaged\", \"cheap\", \"fake\", \"not working\"]\n",
        "\n",
        "# Function to clean and annotate each review\n",
        "def clean_review(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'by .* on \\w+ \\d{2}, \\d{4} Verified Buyer', '', text)\n",
        "    if re.fullmatch(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}|\\w{3,9} \\d{1,2}, \\d{4}', text):\n",
        "        return None\n",
        "    text = re.sub(r'\\b\\w{3,9} \\d{1,2}, \\d{4}\\b', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Function to annotate usefulness of product based on review sentiment\n",
        "def annotate_useful_in_india(review):\n",
        "    if not review:\n",
        "        return \"Maybe\"\n",
        "    review = review.lower()\n",
        "    if any(kw in review for kw in positive_keywords):\n",
        "        return \"Yes\"\n",
        "    elif any(kw in review for kw in negative_keywords):\n",
        "        return \"No\"\n",
        "    else:\n",
        "        return \"Maybe\"\n",
        "\n",
        "# Function to aggregate sentiment for each product\n",
        "def aggregate_product_usefulness(product_reviews):\n",
        "    positive_reviews = 0\n",
        "    negative_reviews = 0\n",
        "    maybe_reviews = 0\n",
        "\n",
        "    for review in product_reviews:\n",
        "        sentiment = annotate_useful_in_india(review)\n",
        "        if sentiment == \"Yes\":\n",
        "            positive_reviews += 1\n",
        "        elif sentiment == \"No\":\n",
        "            negative_reviews += 1\n",
        "        else:\n",
        "            maybe_reviews += 1\n",
        "\n",
        "    if positive_reviews > negative_reviews:\n",
        "        return \"Yes\"\n",
        "    elif negative_reviews > positive_reviews:\n",
        "        return \"No\"\n",
        "    else:\n",
        "        return \"Maybe\"\n",
        "\n",
        "# Group reviews by Product Name\n",
        "grouped_reviews = df.groupby('Product Name')['Reviews'].apply(list).reset_index()\n",
        "\n",
        "# Annotate each product based on its reviews\n",
        "grouped_reviews['Useful_in_India'] = grouped_reviews['Reviews'].apply(lambda reviews: aggregate_product_usefulness(reviews))\n",
        "\n",
        "# Save the annotated results to a new CSV\n",
        "grouped_reviews.to_csv(\"Annotated_Product_Reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# Optionally, download the file if you're working in Colab\n",
        "from google.colab import files\n",
        "files.download(\"Annotated_Product_Reviews.csv\")\n",
        "\n",
        "print(\"✅ Product usefulness annotated and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ytXoJXl_AmjQ",
        "outputId": "e8fca573-0fe3-4788-b7c8-0ada61892c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b03ce617-9d73-4e83-8918-6459f56520b4\", \"Annotated_Product_Reviews.csv\", 36829)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Product usefulness annotated and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rk3scQIFF_L",
        "outputId": "aaefe9ec-8254-4af6-9fa6-40dbcd6b7365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download VADER lexicon (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Snapdeal_Product_Reviews.csv\")\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def get_sentiment(text):\n",
        "    sentiment_score = sia.polarity_scores(str(text))\n",
        "    if sentiment_score['compound'] >= 0.05:\n",
        "        return \"Positive\"\n",
        "    elif sentiment_score['compound'] <= -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df[\"Sentiment\"] = df[\"Reviews\"].apply(get_sentiment)\n",
        "\n",
        "# Aggregate sentiment per product\n",
        "product_sentiment = df.groupby(\"Product Name\")[\"Sentiment\"].value_counts().unstack(fill_value=0)\n",
        "product_sentiment[\"Total Reviews\"] = product_sentiment.sum(axis=1)\n",
        "product_sentiment[\"Positive %\"] = (product_sentiment.get(\"Positive\", 0) / product_sentiment[\"Total Reviews\"]) * 100\n",
        "product_sentiment[\"Negative %\"] = (product_sentiment.get(\"Negative\", 0) / product_sentiment[\"Total Reviews\"]) * 100\n",
        "product_sentiment[\"Neutral %\"] = (product_sentiment.get(\"Neutral\", 0) / product_sentiment[\"Total Reviews\"]) * 100\n",
        "\n",
        "# Classify overall product sentiment\n",
        "def classify_overall_sentiment(row):\n",
        "    if row[\"Positive %\"] > 50:\n",
        "        return \"Liked\"\n",
        "    elif row[\"Negative %\"] > 50:\n",
        "        return \"Not Liked\"\n",
        "    else:\n",
        "        return \"Mixed Opinion\"\n",
        "\n",
        "# Apply classification\n",
        "product_sentiment[\"Overall Sentiment\"] = product_sentiment.apply(classify_overall_sentiment, axis=1)\n",
        "\n",
        "# Save results\n",
        "df.to_csv(\"Reviews_Sentiment_Analysis.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "product_sentiment.to_csv(\"Product_Sentiment_Summary.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Sentiment analysis completed. Results saved to CSV files.\")\n",
        "\n",
        "# Display summary\n",
        "def generate_summary():\n",
        "    \"\"\"Generates a summary of user reactions based on sentiment analysis.\"\"\"\n",
        "    sentiment_counts = df[\"Sentiment\"].value_counts().to_dict()\n",
        "    total_reviews = len(df)\n",
        "    summary = f\"Out of {total_reviews} reviews:\\n\"\n",
        "    for sentiment, count in sentiment_counts.items():\n",
        "        summary += f\"- {sentiment}: {count} reviews ({(count/total_reviews)*100:.2f}%)\\n\"\n",
        "    return summary\n",
        "\n",
        "print(generate_summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOLbOyXjFicN",
        "outputId": "cff477e3-918c-46f7-c487-c3207a64d9d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis completed. Results saved to CSV files.\n",
            "Out of 500 reviews:\n",
            "- Positive: 466 reviews (93.20%)\n",
            "- Neutral: 25 reviews (5.00%)\n",
            "- Negative: 9 reviews (1.80%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Snapdeal_Product_Reviews.csv\")\n",
        "\n",
        "# Define key aspects to analyze\n",
        "aspects = [\"quality\", \"price\", \"delivery\", \"pack\", \"performance\", \"item\", \"product\", \"useful\", \"smooth\",\n",
        "           \"material\", \"design\", \"service\", \"fabric\", \"satisf\", \"sharper\", \"weight\", \"cloth\", \"look\",\n",
        "         \"fit\", \"comfort\", \"size\", \"deal\", \"worth\", \"gas\", \"easy\", \"colour\"]\n",
        "\n",
        "\n",
        "def extract_aspects(text):\n",
        "    \"\"\"Identify key aspects mentioned in the review.\"\"\"\n",
        "    found_aspects = [aspect for aspect in aspects if aspect in text.lower()]\n",
        "    return list(set(found_aspects))  # Ensure unique aspects\n",
        "\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Determine sentiment polarity of a review.\"\"\"\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "    return \"positive\" if sentiment_score > 0.05 else \"negative\" if sentiment_score < -0.05 else \"neutral\"\n",
        "\n",
        "# Apply functions to dataset\n",
        "df['aspects'] = df['Reviews'].astype(str).apply(extract_aspects)\n",
        "df['sentiment'] = df['Reviews'].astype(str).apply(analyze_sentiment)\n",
        "\n",
        "# Convert aspect lists to comma-separated strings to avoid duplication\n",
        "df['aspects'] = df['aspects'].apply(lambda x: ', '.join(x) if x else 'None')\n",
        "\n",
        "# Remove duplicate reviews\n",
        "df = df.drop_duplicates(subset=['Reviews'])\n",
        "\n",
        "# Save results to CSV\n",
        "df.to_csv(\"aspect_based_opinion_mining_results.csv\", index=False)\n",
        "\n",
        "print(\"Aspect-based opinion mining completed. Results saved to aspect_based_opinion_mining_results.csv\")\n",
        "print(\"Total unique reviews in dataset:\", len(df))\n",
        "print(\"Number of reviews with extracted aspects:\", (df['aspects'] != 'None').sum())\n",
        "print(\"Number of reviews with NO aspects:\", (df['aspects'] == 'None').sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbJmhTybGB_7",
        "outputId": "cb0bf56e-ac6b-462d-c189-509fa3b00e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspect-based opinion mining completed. Results saved to aspect_based_opinion_mining_results.csv\n",
            "Total unique reviews in dataset: 492\n",
            "Number of reviews with extracted aspects: 428\n",
            "Number of reviews with NO aspects: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Snapdeal_Product_Reviews.csv\")\n",
        "summary_df = pd.read_csv(\"Product_Sentiment_Summary.csv\")\n",
        "\n",
        "# Define key aspects to analyze\n",
        "aspects = [\"quality\", \"price\", \"delivery\", \"pack\", \"performance\", \"item\", \"useful\", \"smooth\",\n",
        "           \"material\", \"design\", \"service\", \"fabric\", \"satisf\", \"sharper\", \"weight\", \"cloth\", \"look\",\n",
        "           \"fit\", \"comfort\", \"size\", \"deal\", \"worth\", \"gas\", \"easy\", \"colour\"]\n",
        "\n",
        "\n",
        "def extract_aspects(text):\n",
        "    \"\"\"Identify key aspects mentioned in the review.\"\"\"\n",
        "    found_aspects = [aspect for aspect in aspects if aspect in text.lower()]\n",
        "    return list(set(found_aspects))  # Ensure unique aspects\n",
        "\n",
        "\n",
        "# Apply aspect extraction\n",
        "df['aspects'] = df['Reviews'].astype(str).apply(extract_aspects)\n",
        "\n",
        "# Remove duplicate reviews\n",
        "df = df.drop_duplicates(subset=['Reviews'])\n",
        "\n",
        "# Aggregate aspect occurrences per product\n",
        "product_aspect_data = []\n",
        "for _, row in df.iterrows():\n",
        "    for aspect in row['aspects']:\n",
        "        product_aspect_data.append({'Product Name': row['Product Name'], 'Aspect': aspect})\n",
        "\n",
        "aspect_df = pd.DataFrame(product_aspect_data)\n",
        "\n",
        "# Identify best and worst aspects per product\n",
        "best_worst_aspects = []\n",
        "for product, group in aspect_df.groupby('Product Name'):\n",
        "    aspect_counts = Counter(group['Aspect'])\n",
        "    if not aspect_counts:\n",
        "        best_worst_aspects.append({'Product Name': product, 'Best Aspect': 'None', 'Worst Aspect': 'None'})\n",
        "        continue\n",
        "\n",
        "    overall_sentiment = summary_df.loc[summary_df['Product Name'] == product, 'Overall Sentiment'].values[0]\n",
        "\n",
        "    if overall_sentiment == \"Liked\":\n",
        "        max_count = max(aspect_counts.values())\n",
        "        best_aspects = [aspect for aspect, count in aspect_counts.items() if count == max_count]\n",
        "        best_aspect = sorted(best_aspects)[0]  # Pick alphabetically first if tie\n",
        "        worst_aspect = 'None'\n",
        "    else:\n",
        "        max_count = max(aspect_counts.values())\n",
        "        worst_aspects = [aspect for aspect, count in aspect_counts.items() if count == max_count]\n",
        "        worst_aspect = sorted(worst_aspects)[0]  # Pick alphabetically first if tie\n",
        "        best_aspect = 'None'\n",
        "\n",
        "    best_worst_aspects.append({'Product Name': product, 'Best Aspect': best_aspect, 'Worst Aspect': worst_aspect})\n",
        "\n",
        "best_worst_df = pd.DataFrame(best_worst_aspects)\n",
        "\n",
        "# Save results to CSV\n",
        "best_worst_df.to_csv(\"best_worst_aspects_per_product.csv\", encoding=\"utf-8-sig\", index=False)\n",
        "\n",
        "print(\"Aspect-based opinion mining completed. Results saved to CSV files.\")\n",
        "print(\"Total unique reviews in dataset:\", len(df))\n",
        "print(\"Number of reviews with extracted aspects:\", (df['aspects'].str.len() > 0).sum())\n",
        "print(\"Number of reviews with NO aspects:\", (df['aspects'].str.len() == 0).sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wWojoD3N3Lg",
        "outputId": "244bc2e6-c587-41d1-a87d-a36dde346424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspect-based opinion mining completed. Results saved to CSV files.\n",
            "Total unique reviews in dataset: 492\n",
            "Number of reviews with extracted aspects: 306\n",
            "Number of reviews with NO aspects: 186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"Annotated_Product_Reviews.csv\")\n",
        "\n",
        "# Feature Engineering: Adding features like sentiment and aspects\n",
        "# Assuming sentiment column exists and aspects are already extracted\n",
        "df['sentiment_label'] = df['Useful_in_India'].map({'Yes': 1, 'No': 0, 'Maybe': 2})  # Sentiment as numerical values\n",
        "\n",
        "# Create a 'text' feature by combining reviews and aspects (if you need to use both)\n",
        "df['combined_features'] = df['Reviews']\n",
        "\n",
        "# Vectorizing text data (Bag-of-Words representation)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df['combined_features']\n",
        "y = df['sentiment_label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a pipeline with vectorizer and classifier\n",
        "pipeline = make_pipeline(\n",
        "    vectorizer,  # Vectorize the text data\n",
        "    StandardScaler(with_mean=False),  # To handle sparse matrix from CountVectorizer\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42)  # Classifier\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Save the trained model (optional)\n",
        "import joblib\n",
        "joblib.dump(pipeline, 'product_usefulness_model.pkl')\n",
        "\n",
        "print(\"✅ Model training complete and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXOvPhPxRKGG",
        "outputId": "c5321a94-4fb4-43fc-f3c1-43684bcb03b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n",
            "Accuracy: 1.0\n",
            "✅ Model training complete and saved.\n"
          ]
        }
      ]
    }
  ]
}